{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is Logistic Regression, and how does it differ from Linear\n",
        "Regression?\n",
        "Ans:-Logistic Regression:-\n",
        "Logistic Regression is a type of supervised learning algorithm used for classification problems. It's a popular method for predicting the probability of an event occurring based on a set of input variables. The goal is to find the best-fitting model that predicts the probability of the target variable (usually binary, 0 or 1) based on the predictor variables.\n",
        "\n",
        "Key characteristics:\n",
        "\n",
        "* Binary outcome: Logistic Regression is typically used for binary classification problems, where the target variable has two possible outcomes (e.g., 0/1, yes/no, etc.).\n",
        "* Probability prediction: The algorithm predicts the probability of the positive outcome (e.g., probability of a customer buying a product).\n",
        "* Sigmoid function: Logistic Regression uses the sigmoid function (also known as the logistic function) to transform the linear combination of predictor variables into a probability between 0 and 1.\n",
        "\n",
        "Difference from Linear Regression:\n",
        "\n",
        "* Outcome variable: Linear Regression is used for continuous outcome variables, while Logistic Regression is used for binary outcome variables.\n",
        "* Prediction type: Linear Regression predicts a continuous value, while Logistic Regression predicts a probability.\n",
        "* Model formulation: Linear Regression uses a linear equation to model the relationship between the predictor variables and the outcome variable. Logistic Regression uses a logistic function to model the probability of the positive outcome.\n",
        "* Interpretation: In Linear Regression, coefficients represent the change in the outcome variable for a one-unit change in the predictor variable. In Logistic Regression, coefficients represent the change in the log-odds of the positive outcome for a one-unit change in the predictor variable.\n",
        "\n",
        "When to use Logistic Regression:\n",
        "\n",
        "* Binary classification: Use Logistic Regression when you're dealing with a binary classification problem, such as predicting whether a customer will churn or not.\n",
        "* Probability prediction: Use Logistic Regression when you need to predict the probability of an event occurring, such as the probability of a customer responding to a marketing campaign.\n",
        "\n",
        "Common applications:\n",
        "\n",
        "* Credit risk assessment: Logistic Regression is often used in credit risk assessment to predict the probability of a borrower defaulting on a loan.\n",
        "* Marketing: Logistic Regression is used in marketing to predict the probability of a customer responding to a campaign or buying a product.\n",
        "* Medical diagnosis: Logistic Regression is used in medical diagnosis to predict the probability of a patient having a disease based on symptoms and test results.\n",
        "\n",
        "Question 2: Explain the role of the Sigmoid function in Logistic Regression?\n",
        "Ans:-Sigmoid Function:-The sigmoid function, also known as the logistic function, is a mathematical function that plays a crucial role in Logistic Regression. It's used to transform the linear combination of predictor variables into a probability between 0 and 1.\n",
        "\n",
        "Mathematical Representation:-\n",
        "\n",
        "The sigmoid function is represented mathematically as:\n",
        "\n",
        "σ(x) = 1 / (1 + e^(-x))\n",
        "\n",
        "where e is the base of the natural logarithm, and x is the linear combination of predictor variables.\n",
        "\n",
        "Role in Logistic Regression:\n",
        "\n",
        "The sigmoid function serves several purposes in Logistic Regression:\n",
        "\n",
        "* Probability mapping: It maps the linear combination of predictor variables to a probability between 0 and 1, allowing us to interpret the output as a probability.\n",
        "* Non-linearity: The sigmoid function introduces non-linearity into the model, enabling it to capture complex relationships between predictor variables and the target variable.\n",
        "* Binary classification: The sigmoid function is particularly useful for binary classification problems, where the target variable has two possible outcomes (0/1, yes/no, etc.).\n",
        "\n",
        "Properties:\n",
        "\n",
        "The sigmoid function has several important properties:\n",
        "\n",
        "* S-shaped curve: The sigmoid function has an S-shaped curve, where the output starts at 0 and approaches 1 as the input increases.\n",
        "* Monotonic: The sigmoid function is monotonic, meaning that the output always increases as the input increases.\n",
        "* Continuous and differentiable: The sigmoid function is continuous and differentiable, making it suitable for optimization algorithms.\n",
        "\n",
        "Impact on Model:\n",
        "\n",
        "The sigmoid function has a significant impact on the Logistic Regression model:\n",
        "\n",
        "* Probability prediction: The sigmoid function enables the model to predict probabilities, which can be used for decision-making.\n",
        "* Thresholding: The sigmoid function allows us to set a threshold for classification, where values above the threshold are classified as positive, and values below are classified as negative.\n",
        "\n",
        "Common Issues:\n",
        "\n",
        "* Vanishing gradients: The sigmoid function can lead to vanishing gradients during backpropagation, making it challenging to train deep neural networks.\n",
        "* Saturated outputs: The sigmoid function can produce saturated outputs (0 or 1) for large inputs, which can lead to poor model performance.\n",
        "\n",
        "Question 3: What is Regularization in Logistic Regression and why is it needed?\n",
        "Ans:- Regularization in Logistic Regression:\n",
        "\n",
        "Regularization is a technique used in Logistic Regression to prevent overfitting by adding a penalty term to the loss function. Overfitting occurs when a model is too complex and fits the training data too closely, resulting in poor performance on new, unseen data.\n",
        "\n",
        "Why Regularization is Needed:\n",
        "\n",
        "Regularization is needed in Logistic Regression for several reasons:\n",
        "\n",
        "* Prevent Overfitting: Regularization helps to reduce overfitting by adding a penalty term to the loss function, which discourages large weights.\n",
        "* Improve Model Generalizability: By reducing overfitting, regularization improves the model's ability to generalize to new, unseen data.\n",
        "* Reduce Model Complexity: Regularization helps to reduce model complexity by shrinking the weights towards zero, which can improve model interpretability.\n",
        "\n",
        "Types of Regularization:\n",
        "\n",
        "There are two common types of regularization used in Logistic Regression:\n",
        "\n",
        "* L1 Regularization (Lasso): L1 regularization adds a penalty term proportional to the absolute value of the weights. It can set some weights to zero, effectively performing feature selection.\n",
        "* L2 Regularization (Ridge): L2 regularization adds a penalty term proportional to the square of the weights. It reduces the magnitude of the weights but does not set them to zero.\n",
        "\n",
        "How Regularization Works:\n",
        "\n",
        "Regularization works by adding a penalty term to the loss function, which is typically the log-likelihood function in Logistic Regression. The penalty term is proportional to the magnitude of the weights, and the strength of the penalty is controlled by a hyperparameter.\n",
        "\n",
        "Benefits of Regularization:\n",
        "\n",
        "The benefits of regularization in Logistic Regression include:\n",
        "\n",
        "* Improved Model Performance: Regularization can improve the model's performance on new, unseen data by reducing overfitting.\n",
        "* Reduced Risk of Overfitting: Regularization reduces the risk of overfitting by adding a penalty term to the loss function.\n",
        "* Improved Model Interpretability: Regularization can improve model interpretability by reducing the magnitude of the weights and selecting relevant features.\n",
        "\n",
        "Common Regularization Techniques:\n",
        "\n",
        "Some common regularization techniques used in Logistic Regression include:\n",
        "\n",
        "* Cross-Validation: Cross-validation is a technique used to evaluate the model's performance on unseen data and tune the regularization hyperparameter.\n",
        "* Grid Search: Grid search is a technique used to find the optimal value of the regularization hyperparameter.\n",
        "\n",
        "Question 4: What are some common evaluation metrics for classification models, and why are they important?\n",
        "Ans:- Common Evaluation Metrics for Classification Models:\n",
        "\n",
        "* Accuracy: Measures the proportion of correctly classified instances out of all instances in the dataset.\n",
        "* Precision: Measures the proportion of true positives (correctly predicted instances) among all positive predictions made by the model.\n",
        "* Recall: Measures the proportion of true positives among all actual positive instances in the dataset.\n",
        "* F1 Score: The harmonic mean of precision and recall, providing a balanced measure of both.\n",
        "* Area Under the ROC Curve (AUC-ROC): Measures the model's ability to distinguish between positive and negative classes across different threshold settings.\n",
        "* Confusion Matrix: A table used to describe the performance of a classification model, showing the number of true positives, false positives, true negatives, and false negatives.\n",
        "\n",
        "Why These Metrics Are Important:\n",
        "\n",
        "* Assessing Model Performance: These metrics help evaluate how well a classification model performs, identifying its strengths and weaknesses.\n",
        "* Comparing Models: Different models can be compared using these metrics to determine which one performs better for a specific task.\n",
        "* Threshold Selection: Metrics like precision, recall, and F1 score are crucial for determining the optimal threshold for classification, especially in cases where the cost of false positives and false negatives differs.\n",
        "* Understanding Trade-offs: Metrics like precision and recall highlight the trade-offs in model performance, helping to balance false positives and false negatives according to the problem's requirements.\n",
        "* Business Impact: These metrics can be tied to business outcomes, such as revenue, customer satisfaction, or risk management, ensuring that the model aligns with business goals.\n",
        "\n",
        "When to Use Each Metric:\n",
        "\n",
        "* Accuracy: Useful when the classes are balanced and the cost of false positives and false negatives is similar.\n",
        "* Precision and Recall: Important in cases where the cost of false positives or false negatives is high, such as in medical diagnosis or fraud detection.\n",
        "* F1 Score: Useful when there's an imbalance between classes or when both precision and recall are important.\n",
        "* AUC-ROC: Helps assess the model's performance across all possible thresholds, useful for comparing models without committing to a specific threshold.\n",
        "* Confusion Matrix: Provides a detailed view of model performance, useful for understanding specific types of errors and their impact.\n",
        "\n",
        "Real-World Applications:\n",
        "\n",
        "* Healthcare: Metrics like precision and recall are critical in medical diagnosis, where false negatives (missing a diagnosis) or false positives (incorrect diagnosis) can have severe consequences.\n",
        "* Finance: In credit scoring or fraud detection, metrics like precision, recall, and F1 score are used to evaluate the model's ability to identify high-risk individuals or fraudulent transactions.\n",
        "* Marketing: Metrics like accuracy and AUC-ROC are used to assess the effectiveness of targeted marketing campaigns, such as predicting customer churn or response to promotions.\n",
        "\n",
        "Question 5: Write a Python program that loads a CSV file into a Pandas DataFrame,splits into train/test sets, trains a Logistic Regression model, and prints its accuracy.\n",
        "Ans:-\n",
        "\n"
      ],
      "metadata": {
        "id": "UXkPMLCEQLXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn import datasets\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "\n",
        "# Create a Pandas DataFrame\n",
        "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
        "df['target'] = iris.target\n",
        "\n",
        "# Split the data into features (X) and target (y)\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5R5tnF3V5Jq",
        "outputId": "e321912c-bcba-4995-8a09-de784d90a987"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: Write a Python program to train a Logistic Regression model using L2\n",
        "regularization (Ridge) and print the model coefficients and accuracy.\n",
        "Ans:-"
      ],
      "metadata": {
        "id": "0wt9WnzNXzlV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "\n",
        "# Create a Pandas DataFrame\n",
        "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
        "df['target'] = iris.target\n",
        "\n",
        "# We'll use a binary classification problem, so let's convert the target variable to binary\n",
        "df['target'] = df['target'].apply(lambda x: 1 if x == 0 else 0)\n",
        "\n",
        "# Split the data into features (X) and target (y)\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Logistic Regression model with L2 regularization (Ridge)\n",
        "model = LogisticRegression(penalty='l2', C=0.1, max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Print the model coefficients\n",
        "print(\"Model Coefficients:\")\n",
        "for feature, coefficient in zip(X.columns, model.coef_[0]):\n",
        "    print(f\"{feature}: {coefficient:.2f}\")\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nModel Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bt10A3qmZ1qE",
        "outputId": "f5a2deba-51e5-4c55-e074-9097f343a405"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Coefficients:\n",
            "sepal length (cm): -0.32\n",
            "sepal width (cm): 0.37\n",
            "petal length (cm): -1.21\n",
            "petal width (cm): -0.49\n",
            "\n",
            "Model Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr' and print the classification report."
      ],
      "metadata": {
        "id": "R7otCLC_aFmd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "\n",
        "# Create a Pandas DataFrame\n",
        "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
        "df['target'] = iris.target\n",
        "\n",
        "# Split the data into features (X) and target (y)\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Logistic Regression model for multiclass classification using one-vs-rest (OvR)\n",
        "model = LogisticRegression(multi_class='ovr', max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Print the classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qHvATpmaYNy",
        "outputId": "66a46e80-5da6-4ca8-b135-7409e045e133"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      0.89      0.94         9\n",
            "           2       0.92      1.00      0.96        11\n",
            "\n",
            "    accuracy                           0.97        30\n",
            "   macro avg       0.97      0.96      0.97        30\n",
            "weighted avg       0.97      0.97      0.97        30\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to apply GridSearchCV to tune C and penalty\n",
        "hyperparameters for Logistic Regression and print the best parameters and validation\n",
        "accuracy."
      ],
      "metadata": {
        "id": "qnan6_PHa2Cy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "\n",
        "# Create a Pandas DataFrame\n",
        "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
        "df['target'] = iris.target\n",
        "\n",
        "# Split the data into features (X) and target (y)\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'penalty': ['l1', 'l2']\n",
        "}\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5)\n",
        "\n",
        "# Perform grid search on the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters\n",
        "print(\"Best Parameters:\")\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "# Print the best validation accuracy\n",
        "print(f\"Best Validation Accuracy: {grid_search.best_score_:.2f}\")\n",
        "\n",
        "# Train a model with the best parameters and evaluate on the test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Accuracy with Best Parameters: {test_accuracy:.2f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arn7RymkbA1Q",
        "outputId": "9ced2a9c-0702-45be-8d52-2a9c70b19148"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters:\n",
            "{'C': 1, 'penalty': 'l2'}\n",
            "Best Validation Accuracy: 0.97\n",
            "Test Accuracy with Best Parameters: 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
            "15 fits failed out of a total of 30.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "15 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py\", line 63, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [       nan 0.93333333        nan 0.96666667        nan 0.94166667]\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to standardize the features before training Logistic Regression and compare the model's accuracy with and without scaling?\n",
        ""
      ],
      "metadata": {
        "id": "ZQOn9fGEbeaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "\n",
        "# Create a Pandas DataFrame\n",
        "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
        "df['target'] = iris.target\n",
        "\n",
        "# Split the data into features (X) and target (y)\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Logistic Regression model without scaling\n",
        "model_without_scaling = LogisticRegression(max_iter=1000)\n",
        "model_without_scaling.fit(X_train, y_train)\n",
        "y_pred_without_scaling = model_without_scaling.predict(X_test)\n",
        "accuracy_without_scaling = accuracy_score(y_test, y_pred_without_scaling)\n",
        "print(f\"Accuracy without scaling: {accuracy_without_scaling:.2f}\")\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train a Logistic Regression model with scaling\n",
        "model_with_scaling = LogisticRegression(max_iter=1000)\n",
        "model_with_scaling.fit(X_train_scaled, y_train)\n",
        "y_pred_with_scaling = model_with_scaling.predict(X_test_scaled)\n",
        "accuracy_with_scaling = accuracy_score(y_test, y_pred_with_scaling)\n",
        "print(f\"Accuracy with scaling: {accuracy_with_scaling:.2f}\")\n",
        "\n",
        "# Compare the accuracies\n",
        "print(f\"Difference in accuracy: {accuracy_with_scaling - accuracy_without_scaling:.2f}\")\n",
        "if accuracy_with_scaling > accuracy_without_scaling:\n",
        "    print(\"Scaling improves the model's accuracy.\")\n",
        "elif accuracy_with_scaling < accuracy_without_scaling:\n",
        "    print(\"Scaling does not improve the model's accuracy.\")\n",
        "else:\n",
        "    print(\"Scaling does not affect the model's accuracy.\")\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYBb9Ysdb8hj",
        "outputId": "ad5b6c05-65f4-412f-82b2-0c487c0a7e36"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 1.00\n",
            "Accuracy with scaling: 1.00\n",
            "Difference in accuracy: 0.00\n",
            "Scaling does not affect the model's accuracy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you are working at an e-commerce company that wants to\n",
        "predict which customers will respond to a marketing campaign. Given an imbalanced dataset (only 5% of customers respond), describe the approach you’d take to build a Logistic Regression model — including data handling, feature scaling, balancing classes, hyperparameter tuning, and evaluating the model for this real-world business use case\n",
        "Ans:- Approach to Building a Logistic Regression Model for Predicting Customer Response\n",
        "\n",
        "Given the imbalanced dataset, our primary goal is to build a Logistic Regression model that effectively predicts which customers will respond to the marketing campaign. Here's a step-by-step approach to achieve this:\n",
        "\n",
        "Data Handling1. Data Exploration: Understand the dataset, including the features, target variable, and class distribution.\n",
        "2. Data Preprocessing: Handle missing values, encode categorical variables, and transform variables if necessary.\n",
        "\n",
        "Feature Scaling1. Standardization: Scale the features using StandardScaler to ensure that all features are on the same scale, which can improve the model's performance.\n",
        "\n",
        "Balancing Classes1. Oversampling the Minority Class: Use techniques like SMOTE (Synthetic Minority Over-sampling Technique) to oversample the minority class (responders).\n",
        "2. Undersampling the Majority Class: Use techniques like RandomUnderSampler to undersample the majority class (non-responders).\n",
        "3. Class Weighting: Use class weights in the Logistic Regression model to give more importance to the minority class.\n",
        "\n",
        "Hyperparameter Tuning1. Grid Search: Perform grid search over hyperparameters like C (regularization strength), penalty (L1 or L2 regularization), and class weights.\n",
        "2. Cross-Validation: Use cross-validation to evaluate the model's performance on unseen data and prevent overfitting.\n",
        "\n",
        "Evaluating the Model1. Metrics: Use metrics like precision, recall, F1-score, and AUC-ROC to evaluate the model's performance, as accuracy can be misleading in imbalanced datasets.\n",
        "2. Confusion Matrix: Analyze the confusion matrix to understand the model's performance on both classes.\n",
        "\n",
        "Model Interpretation1. Feature Importance: Analyze the coefficients of the Logistic Regression model to understand the importance of each feature in predicting customer response.\n",
        "2. Partial Dependence Plots: Use partial dependence plots to visualize the relationship between each feature and the predicted probability of response.\n",
        "\n",
        "Model Deployment1. Model Deployment: Deploy the model in a production-ready environment, where it can predict customer response probabilities for new customers.\n",
        "2. Model Monitoring: Continuously monitor the model's performance on new data and retrain the model as necessary to maintain its performance.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "I3wFfsFDcdbx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Oversample the minority class using SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Scale the features using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_res)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define hyperparameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'class_weight': ['balanced', None]\n",
        "}\n",
        "\n",
        "# Perform grid search with cross-validation\n",
        "grid_search = GridSearchCV(LogisticRegression(max_iter=1000), param_grid, cv=5, scoring='f1_macro')\n",
        "grid_search.fit(X_train_scaled, y_train_res)\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test_scaled)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVhM5oFbdRnE",
        "outputId": "edcbeee5-6bcd-4a78-c2c1-b13d55963a9a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00         9\n",
            "           2       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
            "30 fits failed out of a total of 60.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "30 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py\", line 63, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [       nan 0.90030095        nan 0.90030095        nan 0.95835955\n",
            "        nan 0.95835955        nan 0.94892454        nan 0.94892454]\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}